{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PsVenom/12-hour-Data-analysis-and-modelling-speedrun/blob/main/full_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QymJSk87PSQP"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRlF2z6WubTc"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po-5ZRJgweou"
      },
      "source": [
        "## Importing transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrzypG41wdiE"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_96u0vxnqrZ"
      },
      "outputs": [],
      "source": [
        "#from tensorflow.python.ops.numpy_ops import np_config\n",
        "#np_config.enable_numpy_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSrlapUJwlH5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel,pipeline\n",
        "summarize = pipeline(\"summarization\", model = \"facebook/bart-large\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8Q8sflfQc3K"
      },
      "outputs": [],
      "source": [
        "def BART_t2t(text):\n",
        " return summarize(text, min_length = 3,max_length = 30, do_sample = False)[0]['summary_text']\n",
        "print(BART_t2t(\"BART model pre-trained on English language. It was introduced in the paper BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension by Lewis et al. and first released in this repository.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OFEdU5yI-OR"
      },
      "outputs": [],
      "source": [
        "print(BART_t2t(\"This is a try\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRQgfvoAw_mP"
      },
      "outputs": [],
      "source": [
        "from transformers import BartTokenizer, BartModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
        "bart_model = BartModel.from_pretrained('facebook/bart-large')\n",
        "def BART_pl(text):\n",
        "\n",
        "  text = BART_t2t(text)\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "  a =  bart_model(**inputs)\n",
        "  return a[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPsXo_E-10DS"
      },
      "outputs": [],
      "source": [
        "BART_pl(\"peepee\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbbgivJqrzf_"
      },
      "outputs": [],
      "source": [
        "from transformers import CLIPProcessor, TFCLIPModel\n",
        "#model_clip = TFCLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "#processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtI72ooHxQfQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "import subprocess\n",
        "pca =  PCA(n_components = 3)\n",
        "def reduce_channels(encoding): #dimensionality reduction using PCA and mean correction\n",
        " array = encoding.detach().cpu().numpy().reshape(-1,1024).T\n",
        " X_new = array - np.mean(array)\n",
        " return pca.fit_transform(X_new)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_L_clip(Xt,Yi): #Xt is a string array, Yi is an image array (both tensorflow). Both their dimensions are (batch_size, [image/text dim])\n",
        " print(\"Yi:\", Yi.shape)\n",
        " batch_size = Yi.shape[0]\n",
        " for i in range(batch_size):\n",
        "  Xt.append(BART_t2t(Xt))\n",
        "  Yi = Yi.numpy()\n",
        "  img = Image.fromarray(Yi)\n",
        "  Yi.append(preprocess_clip(img).unsqueeze(0).to(device))\n",
        " Xt = clip.tokenize([Xt]).to(device)\n",
        " \n",
        " with torch.no_grad():\n",
        "   for i in range(batch_size):\n",
        "    logits_per_image, logits_per_text = clip_model(Yi[i], Xt[i])\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "    loss.append(1- probs[0][0])\n",
        " return loss"
      ],
      "metadata": {
        "id": "iSnNRyFKgRXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQDkWFWB0o5A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from enum import Enum\n",
        "from glob import glob\n",
        "from functools import partial\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow_addons.layers import InstanceNormalization\n",
        "\n",
        "import gdown\n",
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7qBvpoV2iUs"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
        "embeddings = embed([\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"I am a sentence for which I would like to get its embedding\"])\n",
        "\n",
        "print(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UA-pZERxWtw"
      },
      "outputs": [],
      "source": [
        "def text2PCA(word):\n",
        "  a = tf.transpose(reduce_channels(BART_pl(word)))\n",
        "  return tf.reshape(a, [6, 512])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fB92i08mxZXm"
      },
      "outputs": [],
      "source": [
        "text2PCA(\"This is a try for many words in the sequece\").shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRah3dTN1ID3"
      },
      "source": [
        "## Input pipeline for the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTu7u5bY62ER"
      },
      "outputs": [],
      "source": [
        "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                                        trainable=False,\n",
        "                                        name=\"universal_sentence_encoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SJds-px1KFT"
      },
      "outputs": [],
      "source": [
        "inputs = layers.Input(shape=[], dtype=tf.string)\n",
        "pretrained_embeddings = tf_hub_embedding_layer(inputs)\n",
        "dense = tf.keras.layers.Dense(512, activation = \"linear\")(pretrained_embeddings)\n",
        "TEXT_PREP = tf.keras.Model(inputs, dense)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Efs_m841w_K"
      },
      "outputs": [],
      "source": [
        "try_output = TEXT_PREP(tf.constant([\"This is a try for many words in the sequece\"]))\n",
        "try_output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_QHmvMWJXhx"
      },
      "source": [
        "### Load in the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On8fWIu5JhU2"
      },
      "source": [
        "## Custom Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyqxD8x8JpCL"
      },
      "outputs": [],
      "source": [
        "def fade_in(alpha, a, b):\n",
        "    return alpha * a + (1.0 - alpha) * b\n",
        "\n",
        "\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "    return -tf.reduce_mean(y_true * y_pred)\n",
        "\n",
        "\n",
        "def pixel_norm(x, epsilon=1e-8):\n",
        "    return x / tf.math.sqrt(tf.reduce_mean(x**2, axis=-1, keepdims=True) + epsilon)\n",
        "\n",
        "\n",
        "def minibatch_std(input_tensor, epsilon=1e-8):\n",
        "    n, h, w, c = tf.shape(input_tensor)\n",
        "    group_size = tf.minimum(4, n)\n",
        "    x = tf.reshape(input_tensor, [group_size, -1, h, w, c])\n",
        "    group_mean, group_var = tf.nn.moments(x, axes=(0), keepdims=False)\n",
        "    group_std = tf.sqrt(group_var + epsilon)\n",
        "    avg_std = tf.reduce_mean(group_std, axis=[1, 2, 3], keepdims=True)\n",
        "    x = tf.tile(avg_std, [group_size, h, w, 1])\n",
        "    return tf.concat([input_tensor, x], axis=-1)\n",
        "\n",
        "\n",
        "class EqualizedConv(layers.Layer):\n",
        "    def __init__(self, out_channels, kernel=3, gain=2, **kwargs):\n",
        "        super(EqualizedConv, self).__init__(**kwargs)\n",
        "        self.kernel = kernel\n",
        "        self.out_channels = out_channels\n",
        "        self.gain = gain\n",
        "        self.pad = kernel != 1\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.in_channels = input_shape[-1]\n",
        "        initializer = keras.initializers.RandomNormal(mean=0.0, stddev=1.0)\n",
        "        self.w = self.add_weight(\n",
        "            shape=[self.kernel, self.kernel, self.in_channels, self.out_channels],\n",
        "            initializer=initializer,\n",
        "            trainable=True,\n",
        "            name=\"kernel\",\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.out_channels,), initializer=\"zeros\", trainable=True, name=\"bias\"\n",
        "        )\n",
        "        fan_in = self.kernel * self.kernel * self.in_channels\n",
        "        self.scale = tf.sqrt(self.gain / fan_in)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if self.pad:\n",
        "            x = tf.pad(inputs, [[0, 0], [1, 1], [1, 1], [0, 0]], mode=\"REFLECT\")\n",
        "        else:\n",
        "            x = inputs\n",
        "        output = (\n",
        "            tf.nn.conv2d(x, self.scale * self.w, strides=1, padding=\"VALID\") + self.b\n",
        "        )\n",
        "        return output\n",
        "\n",
        "\n",
        "class EqualizedDense(layers.Layer):\n",
        "    def __init__(self, units, gain=2, learning_rate_multiplier=1, **kwargs):\n",
        "        super(EqualizedDense, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.gain = gain\n",
        "        self.learning_rate_multiplier = learning_rate_multiplier\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.in_channels = input_shape[-1]\n",
        "        initializer = keras.initializers.RandomNormal(\n",
        "            mean=0.0, stddev=1.0 / self.learning_rate_multiplier\n",
        "        )\n",
        "        self.w = self.add_weight(\n",
        "            shape=[self.in_channels, self.units],\n",
        "            initializer=initializer,\n",
        "            trainable=True,\n",
        "            name=\"kernel\",\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            shape=(self.units,), initializer=\"zeros\", trainable=True, name=\"bias\"\n",
        "        )\n",
        "        fan_in = self.in_channels\n",
        "        self.scale = tf.sqrt(self.gain / fan_in)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        output = tf.add(tf.matmul(inputs, self.scale * self.w), self.b)\n",
        "        return output * self.learning_rate_multiplier\n",
        "\n",
        "\n",
        "class AddNoise(layers.Layer):\n",
        "    def build(self, input_shape):\n",
        "        n, h, w, c = input_shape[0]\n",
        "        initializer = keras.initializers.RandomNormal(mean=0.0, stddev=1.0)\n",
        "        self.b = self.add_weight(\n",
        "            shape=[1, 1, 1, c], initializer=initializer, trainable=True, name=\"kernel\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, noise = inputs\n",
        "        output = x + self.b * noise\n",
        "        return output\n",
        "\n",
        "\n",
        "class AdaIN(layers.Layer):\n",
        "    def __init__(self, gain=1, **kwargs):\n",
        "        super(AdaIN, self).__init__(**kwargs)\n",
        "        self.gain = gain\n",
        "\n",
        "    def build(self, input_shapes):\n",
        "        x_shape = input_shapes[0]\n",
        "        w_shape = input_shapes[1]\n",
        "\n",
        "        self.w_channels = w_shape[-1]\n",
        "        self.x_channels = x_shape[-1]\n",
        "\n",
        "        self.dense_1 = EqualizedDense(self.x_channels, gain=1)\n",
        "        self.dense_2 = EqualizedDense(self.x_channels, gain=1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, w = inputs\n",
        "        ys = tf.reshape(self.dense_1(w), (-1, 1, 1, self.x_channels))\n",
        "        yb = tf.reshape(self.dense_2(w), (-1, 1, 1, self.x_channels))\n",
        "        return ys * x + yb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9maK0wcIJpKk"
      },
      "outputs": [],
      "source": [
        "def Mapping(num_stages, input_shape=512):\n",
        "    z = layers.Input(shape=(input_shape))\n",
        "    w = pixel_norm(z)\n",
        "    for i in range(8):\n",
        "        w = EqualizedDense(512, learning_rate_multiplier=0.01)(w)\n",
        "        w = layers.LeakyReLU(0.2)(w)\n",
        "    w = tf.tile(tf.expand_dims(w, 1), (1, num_stages, 1))\n",
        "    #print(f\"mapping output shape: {w.shape}\")\n",
        "    return keras.Model(z, w, name=\"mapping\")\n",
        "\n",
        "\n",
        "class Generator:\n",
        "    def __init__(self, start_res_log2, target_res_log2):\n",
        "        self.start_res_log2 = start_res_log2\n",
        "        self.target_res_log2 = target_res_log2\n",
        "        self.num_stages = target_res_log2 - start_res_log2 + 1\n",
        "        # list of generator blocks at increasing resolution\n",
        "        self.g_blocks = []\n",
        "        # list of layers to convert g_block activation to RGB\n",
        "        self.to_rgb = []\n",
        "        # list of noise input of different resolutions into g_blocks\n",
        "        self.noise_inputs = []\n",
        "        # filter size to use at each stage, keys are log2(resolution)\n",
        "        self.filter_nums = {\n",
        "            0: 512,\n",
        "            1: 512,\n",
        "            2: 512,  # 4x4\n",
        "            3: 512,  # 8x8\n",
        "            4: 512,  # 16x16\n",
        "            5: 512,  # 32x32\n",
        "            6: 256,  # 64x64\n",
        "            7: 128,  # 128x128\n",
        "            8: 64,  # 256x256\n",
        "            9: 32,  # 512x512\n",
        "            10: 16,\n",
        "        }  # 1024x1024\n",
        "\n",
        "        start_res = 2 ** start_res_log2\n",
        "        self.input_shape = (start_res, start_res, self.filter_nums[start_res_log2])\n",
        "        self.g_input = layers.Input(self.input_shape, name=\"generator_input\")\n",
        "\n",
        "        for i in range(start_res_log2, target_res_log2 + 1):\n",
        "            filter_num = self.filter_nums[i]\n",
        "            res = 2 ** i\n",
        "            self.noise_inputs.append(\n",
        "                layers.Input(shape=(res, res, 1), name=f\"noise_{res}x{res}\")\n",
        "            )\n",
        "            to_rgb = Sequential(\n",
        "                [\n",
        "                    layers.InputLayer(input_shape=(res, res, filter_num)),\n",
        "                    EqualizedConv(3, 1, gain=1),\n",
        "                ],\n",
        "                name=f\"to_rgb_{res}x{res}\",\n",
        "            )\n",
        "            self.to_rgb.append(to_rgb)\n",
        "            is_base = i == self.start_res_log2\n",
        "            if is_base:\n",
        "                input_shape = (res, res, self.filter_nums[i - 1])\n",
        "            else:\n",
        "                input_shape = (2 ** (i - 1), 2 ** (i - 1), self.filter_nums[i - 1])\n",
        "            g_block = self.build_block(\n",
        "                filter_num, res=res, input_shape=input_shape, is_base=is_base\n",
        "            )\n",
        "            self.g_blocks.append(g_block)\n",
        "\n",
        "    def build_block(self, filter_num, res, input_shape, is_base):\n",
        "        input_tensor = layers.Input(shape=input_shape, name=f\"g_{res}\")\n",
        "        noise = layers.Input(shape=(res, res, 1), name=f\"noise_{res}\")\n",
        "        w = layers.Input(shape=512)\n",
        "        ## v: text input to the generator\n",
        "        v = layers.Input(shape=512)\n",
        "        x = input_tensor\n",
        "\n",
        "        if not is_base:\n",
        "            x = layers.UpSampling2D((2, 2))(x)\n",
        "            x = EqualizedConv(filter_num, 3)(x)\n",
        "\n",
        "        x = AddNoise()([x, noise])\n",
        "        print(f\"Before adding text: x: {x.shape}, w: {w.shape}\")\n",
        "        x = layers.LeakyReLU(0.2)(x)\n",
        "        x = InstanceNormalization()(x)\n",
        "        x = AdaIN()([x, w])\n",
        "        print(\"First AdaIN works!\")\n",
        "        print(f\"After adding: {x.shape}\")\n",
        "\n",
        "        ## Added code block\n",
        "        print(f\"Before adding text: x: {x.shape}, v: {v.shape}\")\n",
        "        x = AdaIN()([x, v])\n",
        "        print(\"Second AdaIN works!\")\n",
        "        print(f\"After adding: {x.shape}\")\n",
        "\n",
        "        x = EqualizedConv(filter_num, 3)(x)\n",
        "        x = AddNoise()([x, noise])\n",
        "        x = layers.LeakyReLU(0.2)(x)\n",
        "        x = InstanceNormalization()(x)\n",
        "        x = AdaIN()([x, w])\n",
        "        print(\"Third AdaIN works!\")\n",
        "        x = AdaIN()([x, v])\n",
        "        print(\"Fourth AdaIN works!\")\n",
        "        return keras.Model([input_tensor, w, noise, v], x, name=f\"genblock_{res}x{res}\")\n",
        "\n",
        "    def grow(self, res_log2):\n",
        "        res = 2 ** res_log2\n",
        "\n",
        "        num_stages = res_log2 - self.start_res_log2 + 1\n",
        "        w = layers.Input(shape=(self.num_stages, 512), name=\"w\")\n",
        "        v = layers.Input(shape=(self.num_stages, 512), name = \"v\")\n",
        "        alpha = layers.Input(shape=(1), name=\"g_alpha\")\n",
        "        x = self.g_blocks[0]([self.g_input, w[:, 0], self.noise_inputs[0], v[:, 0]])\n",
        "\n",
        "        if num_stages == 1:\n",
        "            rgb = self.to_rgb[0](x)\n",
        "        else:\n",
        "            for i in range(1, num_stages - 1):\n",
        "\n",
        "                x = self.g_blocks[i]([x, w[:, i], self.noise_inputs[i], v[:, i]])\n",
        "\n",
        "            old_rgb = self.to_rgb[num_stages - 2](x)\n",
        "            old_rgb = layers.UpSampling2D((2, 2))(old_rgb)\n",
        "\n",
        "            i = num_stages - 1\n",
        "            x = self.g_blocks[i]([x, w[:, i], self.noise_inputs[i], v[:, i]])\n",
        "\n",
        "            new_rgb = self.to_rgb[i](x)\n",
        "\n",
        "            rgb = fade_in(alpha[0], new_rgb, old_rgb)\n",
        "\n",
        "        return keras.Model(\n",
        "            [self.g_input, w, self.noise_inputs, alpha, v],\n",
        "            rgb,\n",
        "            name=f\"generator_{res}_x_{res}\",\n",
        "        )\n",
        "\n",
        "\n",
        "class Discriminator:\n",
        "    def __init__(self, start_res_log2, target_res_log2):\n",
        "        self.start_res_log2 = start_res_log2\n",
        "        self.target_res_log2 = target_res_log2\n",
        "        self.num_stages = target_res_log2 - start_res_log2 + 1\n",
        "        # filter size to use at each stage, keys are log2(resolution)\n",
        "        self.filter_nums = {\n",
        "            0: 512,\n",
        "            1: 512,\n",
        "            2: 512,  # 4x4\n",
        "            3: 512,  # 8x8\n",
        "            4: 512,  # 16x16\n",
        "            5: 512,  # 32x32\n",
        "            6: 256,  # 64x64\n",
        "            7: 128,  # 128x128\n",
        "            8: 64,  # 256x256\n",
        "            9: 32,  # 512x512\n",
        "            10: 16,\n",
        "        }  # 1024x1024\n",
        "        # list of discriminator blocks at increasing resolution\n",
        "        self.d_blocks = []\n",
        "        # list of layers to convert RGB into activation for d_blocks inputs\n",
        "        self.from_rgb = []\n",
        "\n",
        "        for res_log2 in range(self.start_res_log2, self.target_res_log2 + 1):\n",
        "            res = 2 ** res_log2\n",
        "            filter_num = self.filter_nums[res_log2]\n",
        "            from_rgb = Sequential(\n",
        "                [\n",
        "                    layers.InputLayer(\n",
        "                        input_shape=(res, res, 3), name=f\"from_rgb_input_{res}\"\n",
        "                    ),\n",
        "                    EqualizedConv(filter_num, 1),\n",
        "                    layers.LeakyReLU(0.2),\n",
        "                ],\n",
        "                name=f\"from_rgb_{res}\",\n",
        "            )\n",
        "\n",
        "            self.from_rgb.append(from_rgb)\n",
        "\n",
        "            input_shape = (res, res, filter_num)\n",
        "            if len(self.d_blocks) == 0:\n",
        "                d_block = self.build_base(filter_num, res)\n",
        "            else:\n",
        "                d_block = self.build_block(\n",
        "                    filter_num, self.filter_nums[res_log2 - 1], res\n",
        "                )\n",
        "\n",
        "            self.d_blocks.append(d_block)\n",
        "\n",
        "    def build_base(self, filter_num, res):\n",
        "        input_tensor = layers.Input(shape=(res, res, filter_num), name=f\"d_{res}\")\n",
        "        x = minibatch_std(input_tensor)\n",
        "        x = EqualizedConv(filter_num, 3)(x)\n",
        "        x = layers.LeakyReLU(0.2)(x)\n",
        "        x = layers.Flatten()(x)\n",
        "        x = EqualizedDense(filter_num)(x)\n",
        "        x = layers.LeakyReLU(0.2)(x)\n",
        "        x = EqualizedDense(1)(x)\n",
        "        return keras.Model(input_tensor, x, name=f\"d_{res}\")\n",
        "\n",
        "    def build_block(self, filter_num_1, filter_num_2, res):\n",
        "        input_tensor = layers.Input(shape=(res, res, filter_num_1), name=f\"d_{res}\")\n",
        "        x = EqualizedConv(filter_num_1, 3)(input_tensor)\n",
        "        x = layers.LeakyReLU(0.2)(x)\n",
        "        x = EqualizedConv(filter_num_2)(x)\n",
        "        x = layers.LeakyReLU(0.2)(x)\n",
        "        x = layers.AveragePooling2D((2, 2))(x)\n",
        "        return keras.Model(input_tensor, x, name=f\"d_{res}\")\n",
        "\n",
        "    def grow(self, res_log2):\n",
        "        res = 2 ** res_log2\n",
        "        idx = res_log2 - self.start_res_log2\n",
        "        alpha = layers.Input(shape=(1), name=\"d_alpha\")\n",
        "        input_image = layers.Input(shape=(res, res, 3), name=\"input_image\")\n",
        "        x = self.from_rgb[idx](input_image)\n",
        "        x = self.d_blocks[idx](x)\n",
        "        if idx > 0:\n",
        "            idx -= 1\n",
        "            downsized_image = layers.AveragePooling2D((2, 2))(input_image)\n",
        "            y = self.from_rgb[idx](downsized_image)\n",
        "            x = fade_in(alpha[0], x, y)\n",
        "\n",
        "            for i in range(idx, -1, -1):\n",
        "                x = self.d_blocks[i](x)\n",
        "        return keras.Model([input_image, alpha], x, name=f\"discriminator_{res}_x_{res}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br_d0b6u4QNR"
      },
      "outputs": [],
      "source": [
        "def log2(x):\n",
        "    return int(np.log2(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M-vPqT2ugDy"
      },
      "outputs": [],
      "source": [
        "LOSS = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ru_QXSfJuOW"
      },
      "outputs": [],
      "source": [
        "class StyleGAN(tf.keras.Model):\n",
        "    def __init__(self, z_dim=512, target_res=64, start_res=4):\n",
        "        super(StyleGAN, self).__init__()\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        self.target_res_log2 = log2(target_res)\n",
        "        self.start_res_log2 = log2(start_res)\n",
        "        self.current_res_log2 = self.target_res_log2\n",
        "        self.num_stages = self.target_res_log2 - self.start_res_log2 + 1\n",
        "\n",
        "        self.alpha = tf.Variable(1.0, dtype=tf.float32, trainable=False, name=\"alpha\")\n",
        "\n",
        "        self.mapping = Mapping(num_stages=self.num_stages)\n",
        "        self.d_builder = Discriminator(self.start_res_log2, self.target_res_log2)\n",
        "        self.g_builder = Generator(self.start_res_log2, self.target_res_log2)\n",
        "        self.g_input_shape = self.g_builder.input_shape\n",
        "\n",
        "        self.phase = None\n",
        "        self.train_step_counter = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
        "\n",
        "        self.loss_weights = {\"gradient_penalty\": 10, \"drift\": 0.001}\n",
        "\n",
        "    def grow_model(self, res):\n",
        "        tf.keras.backend.clear_session()\n",
        "        res_log2 = log2(res)\n",
        "        self.generator = self.g_builder.grow(res_log2)\n",
        "        self.discriminator = self.d_builder.grow(res_log2)\n",
        "        self.current_res_log2 = res_log2\n",
        "        print(f\"\\nModel resolution:{res}x{res}\")\n",
        "\n",
        "    def compile(\n",
        "        self, steps_per_epoch, phase, res, d_optimizer, g_optimizer, *args, **kwargs\n",
        "    ):\n",
        "        self.loss_weights = kwargs.pop(\"loss_weights\", self.loss_weights)\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "        if res != 2 ** self.current_res_log2:\n",
        "            self.grow_model(res)\n",
        "            self.d_optimizer = d_optimizer\n",
        "            self.g_optimizer = g_optimizer\n",
        "\n",
        "        self.train_step_counter.assign(0)\n",
        "        self.phase = phase\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "        super(StyleGAN, self).compile(*args, **kwargs)\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def generate_noise(self, batch_size):\n",
        "        noise = [\n",
        "            tf.random.normal((batch_size, 2 ** res, 2 ** res, 1))\n",
        "            for res in range(self.start_res_log2, self.target_res_log2 + 1)\n",
        "        ]\n",
        "        return noise\n",
        "\n",
        "    def gradient_loss(self, grad):\n",
        "        loss = tf.square(grad)\n",
        "        loss = tf.reduce_sum(loss, axis=tf.range(1, tf.size(tf.shape(loss))))\n",
        "        loss = tf.sqrt(loss)\n",
        "        loss = tf.reduce_mean(tf.square(loss - 1))\n",
        "        return loss\n",
        "\n",
        "    def img_to_numpy(img):\n",
        "      pass\n",
        "\n",
        "    def train_step(self, data):\n",
        "        real_images, real_text = data\n",
        "        print(\"real_text \", real_text)\n",
        "        print(f\"real_text shape: {real_text.shape}\")\n",
        "        for i, e in enumerate(real_text):\n",
        "          print(i, tf.compat.as_str_any(e))\n",
        "        real_text_string = tf.compat.as_str_any(real_text)\n",
        "\n",
        "        self.train_step_counter.assign_add(1)\n",
        "\n",
        "        if self.phase == \"TRANSITION\":\n",
        "            self.alpha.assign(\n",
        "                tf.cast(self.train_step_counter / self.steps_per_epoch, tf.float32)\n",
        "            )\n",
        "        elif self.phase == \"STABLE\":\n",
        "            self.alpha.assign(1.0)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        alpha = tf.expand_dims(self.alpha, 0)\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        real_labels = tf.ones(batch_size)\n",
        "        fake_labels = -tf.ones(batch_size)\n",
        "\n",
        "        z = tf.random.normal((batch_size, self.z_dim))\n",
        "        const_input = tf.ones(tuple([batch_size] + list(self.g_input_shape)))\n",
        "        noise = self.generate_noise(batch_size)\n",
        "\n",
        "\n",
        "        # generator\n",
        "        with tf.GradientTape() as g_tape:\n",
        "            v = text2PCA(real_text_string)\n",
        "            v = tf.expand_dims(v, axis=0)\n",
        "            print(f\"v: shape in train step {v.shape}\")\n",
        "            w = self.mapping(z)\n",
        "            print(f\"w: shape in train step {w.shape}\")\n",
        "            fake_images = self.generator([const_input, w, noise, alpha, v])\n",
        "            print(\"Generated Images with input shape \", fake_images.shape, type(v))\n",
        "            pred_fake = self.discriminator([fake_images, alpha])\n",
        "            g_loss = wasserstein_loss(real_labels, pred_fake)\n",
        "\n",
        "            trainable_weights = (\n",
        "                self.mapping.trainable_weights + self.generator.trainable_weights\n",
        "            )\n",
        "            gradients = g_tape.gradient(g_loss, trainable_weights) \n",
        "            self.g_optimizer.apply_gradients(zip(gradients, trainable_weights))\n",
        "\n",
        "        # discriminator\n",
        "        with tf.GradientTape() as gradient_tape, tf.GradientTape() as total_tape:\n",
        "            # forward pass\n",
        "            pred_fake = self.discriminator([fake_images, alpha])\n",
        "            print(\"pred_fake \", pred_fake)\n",
        "            pred_real = self.discriminator([real_images, alpha])\n",
        "            print(\"pred_real\", pred_real)\n",
        "\n",
        "            epsilon = tf.random.uniform((batch_size, 1, 1, 1))\n",
        "            interpolates = epsilon * real_images + (1 - epsilon) * fake_images\n",
        "            gradient_tape.watch(interpolates)\n",
        "            pred_fake_grad = self.discriminator([interpolates, alpha])\n",
        "\n",
        "            # calculate losses\n",
        "            loss_fake = wasserstein_loss(fake_labels, pred_fake)\n",
        "            print(\"loss fake: \", loss_fake)\n",
        "            loss_real = wasserstein_loss(real_labels, pred_real)\n",
        "            print(\"loss_real: \", loss_real)\n",
        "            loss_fake_grad = wasserstein_loss(fake_labels, pred_fake_grad)\n",
        "            print(\"real_text_string: \", real_text_string)\n",
        "            print(\"fake images shape\", tf.Variable(fake_images * 255).shape)\n",
        "            print(\"Code works before clip loss\")\n",
        "            l_clip = batch_L_clip(real_text_string,tf.Variable(fake_images * 255)) #.astype(np.uint8)\n",
        "            print(\"L_clip success\")\n",
        "            \n",
        "            # gradient penalty\n",
        "            gradients_fake = gradient_tape.gradient(loss_fake_grad, [interpolates])\n",
        "            gradient_penalty = self.loss_weights[\n",
        "                \"gradient_penalty\"\n",
        "            ] * self.gradient_loss(gradients_fake)\n",
        "\n",
        "            # drift loss\n",
        "            all_pred = tf.concat([pred_fake, pred_real], axis=0)\n",
        "            drift_loss = self.loss_weights[\"drift\"] * tf.reduce_mean(all_pred ** 2)\n",
        "\n",
        "            d_loss = loss_fake + loss_real + gradient_penalty + drift_loss+l_clip\n",
        "            LOSS.append(d_loss)\n",
        "            print(d_loss)\n",
        "\n",
        "            gradients = total_tape.gradient(\n",
        "                d_loss, self.discriminator.trainable_weights\n",
        "            )\n",
        "            self.d_optimizer.apply_gradients(\n",
        "                zip(gradients, self.discriminator.trainable_weights)\n",
        "            )\n",
        "\n",
        "        # Update metrics\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\n",
        "            \"d_loss\": self.d_loss_metric.result(),\n",
        "            \"g_loss\": self.g_loss_metric.result(),\n",
        "        }\n",
        "\n",
        "    def call(self, inputs: dict()):\n",
        "        style_code = inputs.get(\"style_code\", None)\n",
        "        v = inputs.get(\"v\", None)\n",
        "        v = text2PCA(v)\n",
        "        v = tf.expand_dims(v, axis=0)\n",
        "        print(v, v.shape)\n",
        "        z = inputs.get(\"z\", None)\n",
        "        noise = inputs.get(\"noise\", None)\n",
        "        batch_size = inputs.get(\"batch_size\", 1)\n",
        "        alpha = inputs.get(\"alpha\", 1.0)\n",
        "        alpha = tf.expand_dims(alpha, 0)\n",
        "        if style_code is None:\n",
        "            if z is None:\n",
        "                z = tf.random.normal((batch_size, self.z_dim))\n",
        "            style_code = self.mapping(z)\n",
        "\n",
        "        if noise is None:\n",
        "            noise = self.generate_noise(batch_size)\n",
        "\n",
        "        # self.alpha.assign(alpha)\n",
        "        print(\"Code works till here\")\n",
        "        const_input = tf.ones(tuple([batch_size] + list(self.g_input_shape)))\n",
        "        images = self.generator([const_input, style_code, noise, alpha, v])\n",
        "        images = tf.keras.backend.clip((images * 0.5 + 0.5) * 255, 0, 255)\n",
        "\n",
        "        return images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Uqz0O72JxvC"
      },
      "outputs": [],
      "source": [
        "START_RES = 4\n",
        "TARGET_RES = 128\n",
        "\n",
        "style_gan = StyleGAN(start_res=START_RES, target_res=TARGET_RES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS-asxSDLfOZ"
      },
      "source": [
        "### Testing the StyleGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuwbxy0_LYJA"
      },
      "source": [
        "## Loading in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wq4uhCFuM2ol"
      },
      "outputs": [],
      "source": [
        "## Using the MS-COCO dataset for a test run \n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASS3sfKRYiIR"
      },
      "outputs": [],
      "source": [
        "# Download caption annotation files\n",
        "annotation_folder = '/annotations/'\n",
        "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
        "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
        "                                           cache_subdir=os.path.abspath('.'),\n",
        "                                           origin='http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
        "                                           extract=True)\n",
        "  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
        "\n",
        "# Download image files\n",
        "image_folder = '/train2014/'\n",
        "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
        "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin='http://images.cocodataset.org/zips/train2014.zip',\n",
        "                                      extract=True)\n",
        "  PATH = os.path.dirname(image_zip) + image_folder\n",
        "  os.remove(image_zip)\n",
        "else:\n",
        "  PATH = os.path.abspath('.') + image_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmIIT87TYmut"
      },
      "outputs": [],
      "source": [
        "with open('/content/annotations/captions_train2014.json', 'r') as f:\n",
        "    annotations = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el0K5QdlZYFH"
      },
      "outputs": [],
      "source": [
        "# Group all captions together having the same image ID.\n",
        "image_path_to_caption = collections.defaultdict(list)\n",
        "for val in annotations['annotations']:\n",
        "  caption = f\"<start> {val['caption']} <end>\"\n",
        "  image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (val['image_id'])\n",
        "  image_path_to_caption[image_path].append(caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JbDM_vFZZcu"
      },
      "outputs": [],
      "source": [
        "image_paths = list(image_path_to_caption.keys())\n",
        "random.shuffle(image_paths)\n",
        "train_image_paths = image_paths\n",
        "print(len(train_image_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mMjo_kuEMwK"
      },
      "outputs": [],
      "source": [
        "train_captions = []\n",
        "img_name_vector = []\n",
        "\n",
        "for image_path in train_image_paths:\n",
        "  caption_list = image_path_to_caption[image_path]\n",
        "  train_captions.extend(caption_list)\n",
        "  img_name_vector.extend([image_path] * len(caption_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ig7lVO7ZfBs"
      },
      "outputs": [],
      "source": [
        "print(train_captions[0])\n",
        "Image.open(img_name_vector[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-7vBOyAFa-K"
      },
      "source": [
        "### Testing the L_clip function \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPjcFu30Lt52"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Download the dataset\n",
        "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
        "\n",
        "# Prepare the inputs\n",
        "image, class_id = cifar100[3637]\n",
        "image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input)\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "\n",
        "# Pick the top 5 most similar labels for the image\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "values, indices = similarity[0].topk(5)\n",
        "\n",
        "# Print the result\n",
        "print(\"\\nTop predictions:\\n\")\n",
        "for value, index in zip(values, indices):\n",
        "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FComKHpzFfq9"
      },
      "outputs": [],
      "source": [
        "type(train_captions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnNf258jIs6_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from clip import *\n",
        "from PIL import Image\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess_clip = clip.load(\"ViT-B/32\", device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYYUrz83Fi8K"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "try_img = plt.imread(img_name_vector[100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7zj400MP_hS"
      },
      "outputs": [],
      "source": [
        "img_name_vector[100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1GJRXCpRO3i"
      },
      "outputs": [],
      "source": [
        "Image.open(img_name_vector[100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OovTB-K6P3-2"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "image = preprocess(Image.open(img_name_vector[0])).unsqueeze(0).to(device)\n",
        "print(type(Image.open(img_name_vector[0])))\n",
        "text = clip.tokenize([\"A dog\",\"\"]).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image)\n",
        "    text_features = model.encode_text(text)\n",
        "    print(\"image_features \", image_features.shape)\n",
        "    print(\"text_features \", text_features.shape)\n",
        "    \n",
        "    logits_per_image, logits_per_text = model(image, text)\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "print(\"Label probs:\", probs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbsTZV_wNDSZ"
      },
      "outputs": [],
      "source": [
        "def L_clip(Xt,Yi): #Xt is a string array, Yi is an image array (both tensorflow)\n",
        " print(\"Yi:\", Yi.shape)\n",
        " \n",
        " Xt = BART_t2t(Xt)\n",
        " Xt = clip.tokenize([Xt, \"\"]).to(device)\n",
        " Yi = Yi.numpy()\n",
        " img = Image.fromarray(Yi)\n",
        " Yi = preprocess_clip(img).unsqueeze(0).to(device)\n",
        " with torch.no_grad():\n",
        "    logits_per_image, logits_per_text = clip_model(Yi, Xt)\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "    loss = 1- probs\n",
        "    return loss[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRIadrsYaHf1"
      },
      "outputs": [],
      "source": [
        "# def batch_L_clip(Xt, Yi):\n",
        "#   \"\"\"\n",
        "#   Xt: Batch of text labels \n",
        "#   Yi: Batch of Images\n",
        "#   \"\"\"\n",
        "#    Xt = BART_t2t(i for )\n",
        "#  Xt = clip.tokenize([Xt, \"\"]).to(device)\n",
        "#  img = Image.fromarray(Yi.numpy())\n",
        "#  Yi = preprocess_clip(img).unsqueeze(0).to(device)\n",
        "#  with torch.no_grad():\n",
        "#     image_features = clip_model.encode_image(Yi)\n",
        "#     text_features = clip_model.encode_text(Xt)\n",
        "#     logits_per_image, logits_per_text = clip_model(Yi, Xt)\n",
        "#     probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "#     loss = 1- probs\n",
        "#     return loss[0][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mF2zqr0W32Q"
      },
      "outputs": [],
      "source": [
        "print(\"Original caption: \", train_captions[100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3v-EHEaG3P3"
      },
      "outputs": [],
      "source": [
        "batch_sizes = {2: 16, 3: 16, 4: 16, 5: 16, 6: 16, 7: 8, 8: 4, 9: 2, 10: 1}\n",
        "# We adjust the train step accordingly\n",
        "train_step_ratio = {k: batch_sizes[2] / v for k, v in batch_sizes.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ3eQDVyHwZk"
      },
      "outputs": [],
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W59xt1BNHz12"
      },
      "outputs": [],
      "source": [
        "try_img = load_image(img_name_vector[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOa0JZmkIPIT"
      },
      "outputs": [],
      "source": [
        "# Get unique images\n",
        "encode_train = sorted(set(img_name_vector))\n",
        "\n",
        "# Feel free to change batch_size according to your system configuration\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWVMYzbKIwmS"
      },
      "outputs": [],
      "source": [
        "caption_dataset = tf.data.Dataset.from_tensor_slices(train_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF1G8h1GI_bJ"
      },
      "outputs": [],
      "source": [
        "cap_vector = caption_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvrD81soI27O"
      },
      "outputs": [],
      "source": [
        "img_to_cap_vector = collections.defaultdict(list)\n",
        "for img, cap in zip(img_name_vector, cap_vector):\n",
        "  img_to_cap_vector[img].append(cap)\n",
        "\n",
        "# Create training and validation sets using an 80-20 split randomly.\n",
        "img_keys = list(img_to_cap_vector.keys())\n",
        "random.shuffle(img_keys)\n",
        "\n",
        "slice_index = int(len(img_keys)*0.8)\n",
        "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
        "\n",
        "img_name_train = []\n",
        "cap_train = []\n",
        "for imgt in img_name_train_keys:\n",
        "  capt_len = len(img_to_cap_vector[imgt])\n",
        "  img_name_train.extend([imgt] * capt_len)\n",
        "  cap_train.extend(img_to_cap_vector[imgt])\n",
        "\n",
        "img_name_val = []\n",
        "cap_val = []\n",
        "for imgv in img_name_val_keys:\n",
        "  capv_len = len(img_to_cap_vector[imgv])\n",
        "  img_name_val.extend([imgv] * capv_len)\n",
        "  cap_val.extend(img_to_cap_vector[imgv])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dB4LaCjrJMZ3"
      },
      "outputs": [],
      "source": [
        "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6mdbICafJaL"
      },
      "outputs": [],
      "source": [
        "img_dataset = tf.data.Dataset.from_tensor_slices(img_name_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EANCtNgDe9M0"
      },
      "outputs": [],
      "source": [
        "img_vectors = img_dataset.map(load_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qx9BFr8EfX2c"
      },
      "outputs": [],
      "source": [
        "cap_dataset = tf.data.Dataset.from_tensor_slices(cap_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2CzDlPPfwhC"
      },
      "outputs": [],
      "source": [
        "cap_vector = cap_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt6JUy5ANgDq"
      },
      "outputs": [],
      "source": [
        "for i in cap_vector:\n",
        "  print(i)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtKkzSmYf0ZJ"
      },
      "outputs": [],
      "source": [
        "img_vectors, cap_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdfZVc0Aw7h6"
      },
      "outputs": [],
      "source": [
        "for i in cap_vector:\n",
        "  print(tf.compat.as_str_any(i.numpy()))\n",
        "  print(type(tf.compat.as_str_any(i.numpy())))\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSnjOGurxBWk"
      },
      "outputs": [],
      "source": [
        "for i in img_vectors:\n",
        "  print(i.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RuFMHT0Gxcr"
      },
      "outputs": [],
      "source": [
        "def resize_image(res, image):\n",
        "    # only donwsampling, so use nearest neighbor that is faster to run\n",
        "    image = tf.image.resize(\n",
        "        image, (res, res), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR\n",
        "    )\n",
        "    image = tf.cast(image, tf.float32) / 127.5 - 1.0\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92OqYjg3GmR2"
      },
      "outputs": [],
      "source": [
        "def create_dataloader(res):\n",
        "    batch_size = batch_sizes[log2(res)]\n",
        "    dl = img_vectors.map(partial(resize_image, res), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dl = dl.batch(batch_size, drop_remainder=True).prefetch(1).repeat()\n",
        "    dl2 = cap_vector.batch(batch_size, drop_remainder=True).prefetch(1).repeat()\n",
        "    return dl, dl2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y2jCr1ArVi_"
      },
      "outputs": [],
      "source": [
        "def plot_images(images, log2_res, fname=\"\"):\n",
        "    scales = {2: 0.5, 3: 1, 4: 2, 5: 3, 6: 4, 7: 5, 8: 6, 9: 7, 10: 8}\n",
        "    scale = scales[log2_res]\n",
        "\n",
        "    grid_col = min(images.shape[0], int(32 // scale))\n",
        "    grid_row = 1\n",
        "\n",
        "    f, axarr = plt.subplots(\n",
        "        grid_row, grid_col, figsize=(grid_col * scale, grid_row * scale)\n",
        "    )\n",
        "\n",
        "    for row in range(grid_row):\n",
        "        ax = axarr if grid_row == 1 else axarr[row]\n",
        "        for col in range(grid_col):\n",
        "            ax[col].imshow(images[row * grid_col + col])\n",
        "            ax[col].axis(\"off\")\n",
        "    plt.show()\n",
        "    if fname:\n",
        "        f.savefig(fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQutVxCUEedM"
      },
      "outputs": [],
      "source": [
        "## Training the styleGAN\n",
        "def train(\n",
        "    start_res=START_RES,\n",
        "    target_res=TARGET_RES,\n",
        "    steps_per_epoch=5000,\n",
        "    display_images=True,\n",
        "):\n",
        "    opt_cfg = {\"learning_rate\": 1e-3, \"beta_1\": 0.0, \"beta_2\": 0.99, \"epsilon\": 1e-8}\n",
        "\n",
        "    val_batch_size = 16\n",
        "    val_z = tf.random.normal((val_batch_size, style_gan.z_dim))\n",
        "    val_noise = style_gan.generate_noise(val_batch_size)\n",
        "\n",
        "    start_res_log2 = int(np.log2(start_res))\n",
        "    target_res_log2 = int(np.log2(target_res))\n",
        "\n",
        "    for res_log2 in range(start_res_log2, target_res_log2 + 1):\n",
        "        res = 2 ** res_log2\n",
        "        for phase in [\"TRANSITION\", \"STABLE\"]:\n",
        "            if res == start_res and phase == \"TRANSITION\":\n",
        "                continue\n",
        "\n",
        "            train_dl, caption_dl = create_dataloader(res)\n",
        "            dataset = tf.data.Dataset.zip((train_dl, caption_dl))\n",
        "\n",
        "            steps = int(train_step_ratio[res_log2] * steps_per_epoch)\n",
        "\n",
        "            style_gan.compile(\n",
        "                d_optimizer=tf.keras.optimizers.Adam(**opt_cfg),\n",
        "                g_optimizer=tf.keras.optimizers.Adam(**opt_cfg),\n",
        "                loss_weights={\"gradient_penalty\": 10, \"drift\": 0.001},\n",
        "                steps_per_epoch=steps,\n",
        "                res=res,\n",
        "                phase=phase, \n",
        "                run_eagerly=True,\n",
        "            )\n",
        "\n",
        "            prefix = f\"res_{res}x{res}_{style_gan.phase}\"\n",
        "\n",
        "            ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
        "                f\"checkpoints/stylegan_{res}x{res}.ckpt\",\n",
        "                save_weights_only=True,\n",
        "                verbose=0,\n",
        "            )\n",
        "            print(phase)\n",
        "            style_gan.fit(\n",
        "                dataset, epochs=1, steps_per_epoch=steps, callbacks=[ckpt_cb]\n",
        "            )\n",
        "\n",
        "            if display_images:\n",
        "                images = style_gan({\"z\": val_z, \"noise\": val_noise, \"alpha\": 1.0, \"v\": \"A bird\"})\n",
        "                plot_images(images, res_log2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI3thQAVEzY3"
      },
      "outputs": [],
      "source": [
        "tf.config.run_functions_eagerly(True)\n",
        "train(start_res=4, target_res=32, steps_per_epoch=1, display_images=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bc8sWWs_rUKr"
      },
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "images = style_gan({\"v\": \"Bird\"})\n",
        "plot_images(images, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft5kXglJyaBT"
      },
      "outputs": [],
      "source": [
        "plt.imshow(images[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuq3XAwsqxJq"
      },
      "outputs": [],
      "source": [
        "z = tf.random.normal((batch_size, style_gan.z_dim))\n",
        "const_input = tf.ones(tuple([batch_size] + list(style_gan.g_input_shape)))\n",
        "noise = style_gan.generate_noise(batch_size)\n",
        "w = style_gan.mapping(z)\n",
        "v = text2PCA(\"This is a bird\")\n",
        "v = tf.expand_dims(v, axis=0)\n",
        "style_gan.generator([const_input, w, noise, 1.0, v])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws4-2OlLyAf2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8_QHmvMWJXhx",
        "cS-asxSDLfOZ"
      ],
      "name": "Copy of full_run",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}